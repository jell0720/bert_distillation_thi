# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆè‹¥å°šæœªå®‰è£ï¼‰
# pip install transformers datasets torch accelerate

import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset

# -----------------------------------------------------
# 1. è¨­å®š MPS ä½œç‚ºè£ç½®ï¼ˆmacOS M2/M3ï¼‰
# -----------------------------------------------------
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"ğŸ”¥ ä½¿ç”¨çš„è£ç½®: {device}")

# -----------------------------------------------------
# 2. è¼‰å…¥ SST-2 æ•¸æ“šé›†ï¼ˆGLUE benchmarkï¼‰
# -----------------------------------------------------
dataset = load_dataset("glue", "sst2")
train_data = dataset["train"]
val_data = dataset["validation"]

# -----------------------------------------------------
# 3. é è™•ç†æ•¸æ“šï¼ˆTokenizationï¼‰
# -----------------------------------------------------
model_name = "bert-large-uncased"

# è¼‰å…¥ tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# å®šç¾© Tokenization å‡½æ•¸
def preprocess_function(examples):
    return tokenizer(examples["sentence"], truncation=True, padding="max_length", max_length=128)

# æ‡‰ç”¨ Tokenization
train_data = train_data.map(preprocess_function, batched=True)
val_data = val_data.map(preprocess_function, batched=True)

# è¨­å®šæ ¼å¼ä»¥é©æ‡‰ Hugging Face `Trainer`
train_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# -----------------------------------------------------
# 4. è¼‰å…¥ BERT-Large æ¨¡å‹ï¼ˆä¸¦ç§»å‹•åˆ° MPSï¼‰
# -----------------------------------------------------
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)

# -----------------------------------------------------
# 5. è¨­å®šè¨“ç·´åƒæ•¸ï¼ˆæ›´æ–° `eval_strategy`ï¼‰
# -----------------------------------------------------
training_args = TrainingArguments(
    output_dir="./bert-large-sst2",   # æ¨¡å‹å„²å­˜ä½ç½®
    eval_strategy="epoch",            # âœ… ä¿®æ­£ `evaluation_strategy` å·²æ£„ç”¨
    save_strategy="epoch",
    per_device_train_batch_size=4,    # M2 è¨˜æ†¶é«”å°ï¼Œå»ºè­°é™ä½ batch size
    per_device_eval_batch_size=4,
    num_train_epochs=3,               # è¨“ç·´ 3 å€‹ Epoch
    learning_rate=2e-5,               # å­¸ç¿’ç‡
    weight_decay=0.01,                # L2 æ­£å‰‡åŒ–
    optim="adamw_torch",              # âœ… ä½¿ç”¨ PyTorch å…§å»º AdamW å„ªåŒ–å™¨
    logging_dir="./logs",             # è¨˜éŒ„æ—¥èªŒ
    logging_steps=50,                 # æ¯ 50 æ­¥è¨˜éŒ„ä¸€æ¬¡
    report_to="none"                   # é¿å…è‡ªå‹•ä¸Šå‚³åˆ° Hugging Face
)

# -----------------------------------------------------
# 6. è¨­å®š `Trainer` é€²è¡Œå¾®èª¿ï¼ˆFine-tuningï¼‰
# -----------------------------------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

# é–‹å§‹è¨“ç·´
trainer.train()

# -----------------------------------------------------
# 7. æ¸¬è©¦æ¨¡å‹ï¼ˆé©—è­‰æº–ç¢ºç‡ï¼‰
# -----------------------------------------------------
results = trainer.evaluate()
print(f"Fine-tuned BERT-Large æº–ç¢ºç‡: {results['eval_accuracy']:.4f}")

# -----------------------------------------------------
# 8. å„²å­˜å·²è¨“ç·´çš„æ¨¡å‹
# -----------------------------------------------------
model.save_pretrained("fine-tuned-bert-large")
tokenizer.save_pretrained("fine-tuned-bert-large")

print("âœ… æ¨¡å‹å·²å„²å­˜è‡³ 'fine-tuned-bert-large'")