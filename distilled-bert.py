# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆè‹¥å°šæœªå®‰è£ï¼Œå¯ä½¿ç”¨ pip å®‰è£ï¼‰
# pip install transformers datasets torch accelerate tqdm

import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import torch.optim as optim
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm  # é€²åº¦æ¢

# -----------------------------------------------------
# 1. è¨­å®š MPS æˆ– CUDA ä½œç‚ºè£ç½®
# -----------------------------------------------------
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ ä½¿ç”¨çš„è£ç½®: {device}")

# -----------------------------------------------------
# 2. è¼‰å…¥å·²å¾®èª¿çš„æ•™å¸«æ¨¡å‹èˆ‡å­¸ç”Ÿæ¨¡å‹
# -----------------------------------------------------
# Teacher Model: å·²å¾®èª¿çš„ BERT-Largeï¼ˆSST-2ï¼‰ï¼Œå¦‚æœæ˜¯trainer_teacher.py æ‡‰è©²æ˜¯fine-tuned-bert-large
teacher_model_name = "yoshitomo-matsubara/bert-large-uncased-sst2"
student_model_name = "bert-base-uncased"

# è¼‰å…¥æ•™å¸«æ¨¡å‹
teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_name, num_labels=2).to(device)
teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)

# è¼‰å…¥å­¸ç”Ÿæ¨¡å‹
student_model = AutoModelForSequenceClassification.from_pretrained(student_model_name, num_labels=2).to(device)
student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)

# è¨­å®šæ•™å¸«æ¨¡å‹ç‚ºæ¨è«–æ¨¡å¼ï¼ˆä¸é€²è¡Œè¨“ç·´ï¼‰
teacher_model.eval()

# -----------------------------------------------------
# 3. è¼‰å…¥ä¸¦é è™•ç† SST-2 æ•¸æ“šé›†
# -----------------------------------------------------
dataset = load_dataset("stanfordnlp/sst2")
train_data = dataset["train"]
val_data = dataset["validation"]

# Tokenization è™•ç†
def preprocess_function(examples):
    return teacher_tokenizer(examples["sentence"], truncation=True, padding="max_length", max_length=128)

# æ‡‰ç”¨ Tokenization
train_data = train_data.map(preprocess_function, batched=True)
val_data = val_data.map(preprocess_function, batched=True)

# è¨­å®šæ ¼å¼ä»¥é©æ‡‰ PyTorch
train_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# -----------------------------------------------------
# 4. è¨ˆç®—æ•™å¸«æ¨¡å‹çš„ Soft Labels
# -----------------------------------------------------
# å»ºç«‹ DataLoaderï¼Œæ‰¹æ¬¡å¤§å° 16
train_loader = DataLoader(train_data, batch_size=16, shuffle=False)

def get_teacher_logits(model, dataloader):
    logits_list = []
    with torch.no_grad():
        # åŠ å…¥ tqdm é€²åº¦æ¢
        for batch in tqdm(dataloader, desc="è¨ˆç®—æ•™å¸« Soft Labels"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits_list.append(outputs.logits)
    return torch.cat(logits_list)

# å–å¾—æ•™å¸«æ¨¡å‹é æ¸¬çš„ logits èˆ‡ Soft Labels
teacher_logits = get_teacher_logits(teacher_model, train_loader)
temperature = 2.0
teacher_soft_labels = F.softmax(teacher_logits / temperature, dim=-1)

# -----------------------------------------------------
# 5. å»ºç«‹è¨“ç·´ç”¨çš„ TensorDataset èˆ‡ DataLoader
# -----------------------------------------------------
# é‡æ–°çµ„åˆè¨“ç·´è³‡æ–™ï¼ŒåŠ ä¸Š Soft Labels
train_tensor_dataset = TensorDataset(
    train_data["input_ids"],
    train_data["attention_mask"],
    train_data["label"],
    teacher_soft_labels
)

val_tensor_dataset = TensorDataset(
    val_data["input_ids"],
    val_data["attention_mask"],
    val_data["label"]
)

train_dataloader = DataLoader(train_tensor_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_tensor_dataset, batch_size=16, shuffle=False)

# -----------------------------------------------------
# 6. å®šç¾©è’¸é¤¾æå¤±å‡½æ•¸
# -----------------------------------------------------
def distillation_loss(student_logits, teacher_soft_labels, hard_labels, alpha=0.5, temperature=2.0):
    # KL æ•£åº¦æå¤±
    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)
    kl_loss = F.kl_div(student_log_probs, teacher_soft_labels, reduction="batchmean")
    # äº¤å‰ç†µæå¤±
    ce_loss = F.cross_entropy(student_logits, hard_labels)
    return alpha * kl_loss + (1 - alpha) * ce_loss

# -----------------------------------------------------
# 7. è¨“ç·´å­¸ç”Ÿæ¨¡å‹
# -----------------------------------------------------
student_model.train()
optimizer = optim.AdamW(student_model.parameters(), lr=5e-5)
num_epochs = 3
alpha = 0.5

for epoch in range(num_epochs):
    total_loss = 0
    # tqdm é¡¯ç¤ºæ¯å€‹ epoch çš„é€²åº¦
    for batch in tqdm(train_dataloader, desc=f"è¨“ç·´ Epoch {epoch+1}"):
        optimizer.zero_grad()
        input_ids, attention_mask, labels, soft_labels = [b.to(device) for b in batch]
        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)
        student_logits = outputs.logits
        loss = distillation_loss(student_logits, soft_labels, labels, alpha=alpha, temperature=temperature)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch+1}/{num_epochs}, å¹³å‡æå¤±: {avg_loss:.4f}")

# -----------------------------------------------------
# 8. æ¸¬è©¦å­¸ç”Ÿæ¨¡å‹ï¼ˆé©—è­‰æº–ç¢ºç‡ï¼‰
# -----------------------------------------------------
student_model.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch in tqdm(val_dataloader, desc="é©—è­‰å­¸ç”Ÿæ¨¡å‹"):
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=-1)
        correct += (predictions == labels).sum().item()
        total += labels.size(0)
accuracy = correct / total
print(f"å­¸ç”Ÿæ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„æº–ç¢ºç‡: {accuracy:.4f}")

# -----------------------------------------------------
# 9. å„²å­˜è’¸é¤¾å¾Œçš„å­¸ç”Ÿæ¨¡å‹
# -----------------------------------------------------
student_model.save_pretrained("distilled-bert")
student_tokenizer.save_pretrained("distilled-bert")
print("âœ… è’¸é¤¾å¾Œçš„ BERT æ¨¡å‹å·²å„²å­˜è‡³ 'distilled-bert'")