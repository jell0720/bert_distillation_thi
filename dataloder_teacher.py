# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆè‹¥å°šæœªå®‰è£ï¼‰
# pip install transformers datasets torch accelerate scikit-learn

import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import os

# -----------------------------------------------------
# 1. è¨­å®šè£ç½®ï¼ˆå„ªå…ˆä½¿ç”¨ GPUï¼Œå…¶æ¬¡ MPSï¼Œæœ€å¾Œ CPUï¼‰
# -----------------------------------------------------
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
print(f"ğŸ”¥ ä½¿ç”¨çš„è£ç½®: {device}")

# -----------------------------------------------------
# 2. è¼‰å…¥ SST-2 æ•¸æ“šé›†ï¼ˆå¢åŠ å¿«å–åŠŸèƒ½ï¼‰
# -----------------------------------------------------
cache_dir = "./dataset_cache"
os.makedirs(cache_dir, exist_ok=True)
dataset = load_dataset("stanfordnlp/sst2", cache_dir=cache_dir)
train_data = dataset["train"]
val_data = dataset["validation"]

# -----------------------------------------------------
# 3. é è™•ç†æ•¸æ“šï¼ˆTokenization å„ªåŒ–ï¼‰
# -----------------------------------------------------
model_name = "bert-large-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    # å‹•æ…‹å¡«å……è€Œéå›ºå®šé•·åº¦ï¼Œæé«˜æ•ˆç‡
    return tokenizer(
        examples["sentence"],
        truncation=True,
        padding=False,  # åœ¨ DataCollator ä¸­é€²è¡Œå‹•æ…‹å¡«å……
        max_length=128
    )

# ä½¿ç”¨å¤šé€²ç¨‹åŠ é€Ÿæ•¸æ“šé è™•ç†
train_data = train_data.map(
    preprocess_function, 
    batched=True, 
    num_proc=4,  # ä½¿ç”¨å¤šé€²ç¨‹åŠ é€Ÿ
    desc="æ­£åœ¨è™•ç†è¨“ç·´æ•¸æ“š"
)
val_data = val_data.map(
    preprocess_function, 
    batched=True, 
    num_proc=4,
    desc="æ­£åœ¨è™•ç†é©—è­‰æ•¸æ“š"
)

# è¨­å®šè³‡æ–™æ ¼å¼ç‚º PyTorch tensorsï¼Œä¸¦ç§»é™¤ä¸å¿…è¦çš„åˆ—ä»¥ç¯€çœè¨˜æ†¶é«”
columns = ["input_ids", "attention_mask", "label"]
train_data.set_format(type="torch", columns=columns)
val_data.set_format(type="torch", columns=columns)

# -----------------------------------------------------
# 4. è¼‰å…¥ BERT-Large æ¨¡å‹ï¼ˆå¢åŠ è¨˜æ†¶é«”å„ªåŒ–é¸é …ï¼‰
# -----------------------------------------------------
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=2,
    torch_dtype=torch.float16 if device.type == "cuda" else torch.float32  # åªåœ¨ CUDA è¨­å‚™ä¸Šä½¿ç”¨åŠç²¾åº¦æµ®é»æ•¸
)
model.to(device)

# -----------------------------------------------------
# 5. å®šç¾©æ›´å…¨é¢çš„è©•ä¼°æŒ‡æ¨™å‡½å¼
# -----------------------------------------------------
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average='weighted'),
        "precision": precision_score(labels, predictions, average='weighted'),
        "recall": recall_score(labels, predictions, average='weighted')
    }

# -----------------------------------------------------
# 6. è¨­å®šè¨“ç·´åƒæ•¸èˆ‡å»ºæ§‹ Trainer ç‰©ä»¶ï¼ˆå„ªåŒ–è¨“ç·´è¨­å®šï¼‰
# -----------------------------------------------------
training_args = TrainingArguments(
    output_dir="./bert-large-sst2",
    eval_strategy="epoch",   # æ¯å€‹ epoch çµæŸå¾Œåšé©—è­‰
    save_strategy="epoch",         # æ¯å€‹ epoch çµæŸå¾Œå„²å­˜æ¨¡å‹
    per_device_train_batch_size=8,  # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œæé«˜è¨“ç·´æ•ˆç‡
    per_device_eval_batch_size=16,  # è©•ä¼°æ™‚å¯ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    optim="adamw_torch",
    logging_dir='./logs',
    logging_steps=50,
    fp16=device.type == "cuda",  # åªåœ¨ CUDA è¨­å‚™ä¸Šå•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´
    gradient_accumulation_steps=2,  # æ¢¯åº¦ç´¯ç©ï¼Œå¯ä½¿ç”¨æ›´å¤§çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    warmup_ratio=0.1,  # å­¸ç¿’ç‡é ç†±
    load_best_model_at_end=True,  # è¨“ç·´çµæŸå¾Œè¼‰å…¥æœ€ä½³æ¨¡å‹
    metric_for_best_model="accuracy",  # ä»¥æº–ç¢ºç‡ç‚ºæ¨™æº–é¸æ“‡æœ€ä½³æ¨¡å‹
    save_total_limit=2,  # åªä¿å­˜æœ€è¿‘çš„å…©å€‹æª¢æŸ¥é»ï¼Œç¯€çœç£ç¢Ÿç©ºé–“
)

# ä½¿ç”¨å‹•æ…‹å¡«å……çš„æ•¸æ“šæ•´ç†å™¨ï¼Œæé«˜è¨˜æ†¶é«”ä½¿ç”¨æ•ˆç‡
data_collator = DataCollatorWithPadding(tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# -----------------------------------------------------
# 7. è¨“ç·´èˆ‡è©•ä¼°æ¨¡å‹
# -----------------------------------------------------
trainer.train()
eval_result = trainer.evaluate()
print(f"è©•ä¼°çµæœ: {eval_result}")

# -----------------------------------------------------
# 8. å„²å­˜å·²è¨“ç·´æ¨¡å‹èˆ‡ tokenizer
# -----------------------------------------------------
model.save_pretrained("fine-tuned-bert-large")
tokenizer.save_pretrained("fine-tuned-bert-large")
print("âœ… æ¨¡å‹å·²å„²å­˜è‡³ 'fine-tuned-bert-large'")

# -----------------------------------------------------
# 9. é‡‹æ”¾è¨˜æ†¶é«”
# -----------------------------------------------------
del model
del trainer
torch.cuda.empty_cache() if torch.cuda.is_available() else None
print("ğŸ§¹ å·²é‡‹æ”¾è¨˜æ†¶é«”è³‡æº")